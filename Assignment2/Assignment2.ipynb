{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIgM6C9HYUhm"
      },
      "source": [
        "# Context-sensitive Spelling Correction\n",
        "\n",
        "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
        "\n",
        "Submit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n",
        "\n",
        "Useful links:\n",
        "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
        "- [Norvig's dataset](https://norvig.com/big.txt)\n",
        "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
        "\n",
        "Grading:\n",
        "- 60 points - Implement spelling correction\n",
        "- 20 points - Justify your decisions\n",
        "- 20 points - Evaluate on a test set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-vb8yFOGRDF"
      },
      "source": [
        "## Implement context-sensitive spelling correction\n",
        "\n",
        "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
        "\n",
        "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
        "\n",
        "You may also want to implement:\n",
        "- spell-checking for a concrete language - Russian, Tatar, etc. - any one you know, such that the solution accounts for language specifics,\n",
        "- some recent (or not very recent) paper on this topic,\n",
        "- solution which takes into account keyboard layout and associated misspellings,\n",
        "- efficiency improvement to make the solution faster,\n",
        "- any other idea of yours to improve the Norvigâ€™s solution.\n",
        "\n",
        "IMPORTANT:  \n",
        "Your project should not be a mere code copy-paste from somewhere. You must provide:\n",
        "- Your implementation\n",
        "- Analysis of why the implemented approach is suggested\n",
        "- Improvements of the original approach that you have chosen to implement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "metadata": {
        "id": "jc-o9Q2flO_A"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 218,
      "metadata": {
        "id": "Tde6cXFIoSZI"
      },
      "outputs": [],
      "source": [
        "WORDS = set() #dictionary of words\n",
        "\n",
        "#read bigrams\n",
        "bigram = dict()\n",
        "with open('bigrams.txt', mode = 'rb') as f:\n",
        "  content = f.read()\n",
        "  s = str(content)\n",
        "  w = s[2:-1].replace('\\\\t',' ').replace('\\\\r\\\\n',' ').split()\n",
        "  for i in range(0, len(w), 3):\n",
        "    if not (w[i+1].lower() in bigram):\n",
        "      bigram[w[i+1].lower()] = dict()\n",
        "    bigram[w[i+1].lower()][w[i+2].lower()] = int(w[i])\n",
        "    WORDS.add(w[i+1].lower())\n",
        "    WORDS.add(w[i+2].lower())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#count all words\n",
        "all_words = 0\n",
        "for w in bigram:\n",
        "  all_words += sum(bigram[w].values())\n",
        "all_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gaz0U0SMDZ8J",
        "outputId": "4df0cbf2-9995-40e5-fc5e-f37728c52112"
      },
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "286758206"
            ]
          },
          "metadata": {},
          "execution_count": 219
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pos(ps):\n",
        "  rez = ps[:2]\n",
        "  return rez"
      ],
      "metadata": {
        "id": "O-pw9QBSQEqI"
      },
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LINKS = dict()\n",
        "with open('coca_all_links.txt', mode = 'rb') as f:\n",
        "  content = f.read()\n",
        "  w = str(content)\n",
        "  w = w[2:-5].split('\\\\r\\\\n')\n",
        "  w[78801] = '28\\\\thuman\\\\tsinfulness\\\\tjj\\\\tnn1' # it was broken inside doc:(\n",
        "  w[117501]= '50\\\\trural\\\\tenvironment\\\\tjj\\\\tnn1'\n",
        "  for l in w:\n",
        "    spl = l.split('\\\\t')\n",
        "    if (len(spl) != 5):\n",
        "      continue\n",
        "    count = int(spl[0])\n",
        "    word1 = spl[1].lower()\n",
        "    word2 = spl[2].lower()\n",
        "    prt1 = pos(spl[3])\n",
        "    prt2 = pos(spl[4])\n",
        "    union1 = (word1, prt1)\n",
        "    union2 = (word2, prt2)\n",
        "    if not(union1 in LINKS):\n",
        "      LINKS[union1] = dict()\n",
        "    LINKS[union1][union2] = count"
      ],
      "metadata": {
        "id": "WRNMHNPi9pkY"
      },
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def known(words):\n",
        "    #subset of subwords from WORDS dictionary\n",
        "    return set(w for w in words if w in WORDS)"
      ],
      "metadata": {
        "id": "um25McaX74IQ"
      },
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def edits1(word):\n",
        "    #All edits that are one edit away from `word`.\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "def edits2(word):\n",
        "    #All edits that are two edits away from `word`.\n",
        "    return set(e2 for e1 in edits1(word) for e2 in edits1(e1))"
      ],
      "metadata": {
        "id": "Hu-nmG2_sUk8"
      },
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "metadata": {
        "id": "F-rRJyYFlT7L"
      },
      "outputs": [],
      "source": [
        "# function for fixing misclicks\n",
        "neighbor = {'a': ['q', 's', 'w', 'z'],\n",
        "            'b': ['g', 'h', 'n', 'v'],\n",
        "            'c': ['d', 'f', 'v', 'x'],\n",
        "            'd': ['c', 'e', 'f', 'r', 's', 'x'],\n",
        "            'e': ['d', 'r', 's', 'w'],\n",
        "            'f': ['c', 'd', 'g', 'r', 't', 'v'],\n",
        "            'g': ['b', 'f', 'h', 't', 'v', 'y'],\n",
        "            'h': ['b', 'g', 'j', 'n', 'u', 'y'],\n",
        "            'i': ['j', 'k', 'o', 'u'],\n",
        "            'j': ['h', 'i', 'k', 'm', 'n', 'u'],\n",
        "            'k': ['i', 'j', 'l', 'm', 'o'],\n",
        "            'l': ['k', 'o', 'p'],\n",
        "            'm': ['j', 'k', 'n'],\n",
        "            'n': ['b', 'h', 'j', 'm'],\n",
        "            'o': ['i', 'k', 'l', 'p'],\n",
        "            'p': ['l', 'o'],\n",
        "            'q': ['a', 'w'],\n",
        "            'r': ['d', 'e', 'f', 't'],\n",
        "            's': ['a', 'd', 'e', 'w', 'x', 'z'],\n",
        "            't': ['f', 'g', 'r', 'y'],\n",
        "            'u': ['h', 'i', 'j', 'y'],\n",
        "            'v': ['b', 'c', 'f', 'g'],\n",
        "            'w': ['a', 'e', 'q', 's'],\n",
        "            'x': ['c', 'd', 's', 'z'],\n",
        "            'y': ['g', 'h', 't', 'u'],\n",
        "            'z': ['a', 's', 'x'],\n",
        "            '1': ['q'],\n",
        "            '2': ['q', 'w'],\n",
        "            '3': ['e', 'w'],\n",
        "            '4': ['e', 'r'],\n",
        "            '5': ['r', 't'],\n",
        "            '6': ['t', 'y'],\n",
        "            '7': ['u', 'y'],\n",
        "            '8': ['i', 'u'],\n",
        "            '9': ['i', 'o'],\n",
        "            '0': ['o', 'p'],\n",
        "            '-': ['p'],\n",
        "            '.': ['l'],\n",
        "            ',': ['l', 'm'],\n",
        "            '[': ['p'],\n",
        "            ';': ['l', 'p'],\n",
        "}\n",
        "def edit_mis(word):\n",
        "  splits = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "  replaces = [L + c + R[1:] for L, R in splits if R for c in neighbor[R[0]]]\n",
        "  return known(replaces)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_bi_prob(first_w, second_w):\n",
        "  if not (first_w in bigram and second_w in bigram[first_w]):\n",
        "    return 1.0e-12\n",
        "  count = bigram[first_w][second_w]\n",
        "  N = sum(bigram[first_w].values())\n",
        "  prob = count / N\n",
        "  return prob"
      ],
      "metadata": {
        "id": "yN-diRf9-3A8"
      },
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cmp(word):\n",
        "  if not(word in bigram): return 0\n",
        "  return sum(bigram[word].values())/all_words"
      ],
      "metadata": {
        "id": "pmGl_zGCAgUk"
      },
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def best_variant(word_list, prev_word=None, next_word=None, k=-1):\n",
        "  if (k == -1): k = len(word_list)\n",
        "  counts = []\n",
        "  if (prev_word == None and next_word == None):\n",
        "    counts = [(cmp(word), word) for word in word_list]\n",
        "  elif (prev_word == None):\n",
        "    for word in word_list:\n",
        "      prob = count_bi_prob(word, next_word)\n",
        "      if (prob > 1e-24):\n",
        "        counts.append((prob, word))\n",
        "  elif (next_word == None):\n",
        "    for word in word_list:\n",
        "      prob = count_bi_prob(prev_word, word)\n",
        "      if (prob > 1e-24):\n",
        "        counts.append((prob, word))\n",
        "  else:\n",
        "    for word in word_list:\n",
        "      prob = count_bi_prob(prev_word, word)*count_bi_prob(word, next_word)\n",
        "      if (prob > 1e-24):\n",
        "        counts.append((prob, word))\n",
        "  counts.sort(reverse=True)\n",
        "  return set(counts[:k])"
      ],
      "metadata": {
        "id": "sfEXJ5bqG0Zu"
      },
      "execution_count": 266,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def link_pos(part): #this is because diferences in notation\n",
        "  if (part[:3] == 'ADJ'):\n",
        "    return 'jj'\n",
        "  elif (part[:4] == 'VERB'):\n",
        "    return 'vv'\n",
        "  elif (part[:4] == 'NOUN'):\n",
        "    return 'nn'\n",
        "  else:\n",
        "    return 'th' #other parts of speach"
      ],
      "metadata": {
        "id": "mNUXzeZCak0K"
      },
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def concate_string(start, word, end):\n",
        "  rez = ''\n",
        "  for w in start:\n",
        "    rez = rez + w + ' '\n",
        "  rez = rez + word\n",
        "  if (len(end) == 0): return rez\n",
        "  for w in end:\n",
        "    rez = rez + ' ' + w\n",
        "  return rez"
      ],
      "metadata": {
        "id": "-VCdyuZebq4j"
      },
      "execution_count": 229,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_link_prob(first_w, second_w):\n",
        "  if not (first_w in LINKS and second_w in LINKS[first_w]):\n",
        "    return 1.0e-12\n",
        "  count = LINKS[first_w][second_w]\n",
        "  N = sum(LINKS[first_w].values())\n",
        "  prob = count / N\n",
        "  return prob"
      ],
      "metadata": {
        "id": "ARhnU_OZJt4b"
      },
      "execution_count": 230,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def choose_word(weighted, start, end, k=3):\n",
        "  result_prob = []\n",
        "  for word in weighted:\n",
        "    prob = weighted[word]\n",
        "\n",
        "    words_pos = []\n",
        "    doc = nlp(concate_string(start, word, end))\n",
        "    for token in doc:\n",
        "      words_pos.append((token.text, link_pos(token.pos_)))\n",
        "      if (token.text == word):\n",
        "        unit_w = (token.text, link_pos(token.pos_))\n",
        "\n",
        "    st = True\n",
        "    for unit in words_pos:\n",
        "      if (unit == unit_w):\n",
        "        st = False\n",
        "        continue\n",
        "      if (st):\n",
        "        prob = prob * count_link_prob(unit, unit_w)\n",
        "      else:\n",
        "        prob = prob * count_link_prob(unit_w, unit)\n",
        "\n",
        "    result_prob.append((prob, word))\n",
        "  result_prob.sort(reverse=True)\n",
        "  return result_prob[:k]"
      ],
      "metadata": {
        "id": "snc_35RL548q"
      },
      "execution_count": 300,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def correct_sentence(sentence, poss_k=3):\n",
        "  words = sentence.lower().split()\n",
        "  k = known(words)\n",
        "  if len(k) == len(words):\n",
        "    return concate_string([],words[0],words[1:])\n",
        "  new_sentence = ''\n",
        "  for i in range(len(words)):\n",
        "    if (words[i] in k):\n",
        "      new_sentence += words[i] + ' '\n",
        "      continue\n",
        "    #find unknown word\n",
        "    word = words[i]\n",
        "    prev_word = None if i == 0 else words[i-1]\n",
        "    next_word = None if i == len(words)-1 else words[i+1]\n",
        "\n",
        "    misses = best_variant(edit_mis(word), prev_word, next_word)\n",
        "    edit1 = best_variant(edits1(word), prev_word, next_word)\n",
        "    edit2 = best_variant(edits2(word), prev_word, next_word)\n",
        "\n",
        "    weighted = dict()\n",
        "    for (c, word) in misses:\n",
        "      weighted[word] = c*0.9\n",
        "    for (c, word) in edit1:\n",
        "      if not(word in weighted):\n",
        "        weighted[word] = c*0.8\n",
        "    for (c, word) in edit2:\n",
        "      if not(word in weighted):\n",
        "        weighted[word] = c*0.16\n",
        "\n",
        "    possible_replaces = choose_word(weighted, words[:i], words[i+1:],poss_k)\n",
        "    if (len(possible_replaces) > 0):\n",
        "      new_word = possible_replaces[0][1]\n",
        "    else:\n",
        "      new_word = word\n",
        "      WORDS.add(word) # if it complitly new word (assume it has no mistakes)\n",
        "    new_sentence += new_word + ' '\n",
        "    if (poss_k != 1):\n",
        "      print(f\"Possible replaces:\")\n",
        "      for i in range(len(possible_replaces)):\n",
        "        print(f\"  {possible_replaces[i][1]} ({possible_replaces[i][0]})\")\n",
        "  return new_sentence"
      ],
      "metadata": {
        "id": "ljqMZzAY7SqG"
      },
      "execution_count": 335,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correct_sentence('I am dking sport')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "WhU7ussVGIod",
        "outputId": "ba12c8de-73d6-4457-f71b-7e0653285d14"
      },
      "execution_count": 333,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Possible replaces:\n",
            "  doing (5.49719297400188e-51)\n",
            "  going (4.50666216899983e-51)\n",
            "  dying (7.035635978122063e-52)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'i am doing sport '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 333
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct_sentence(\"The wall was on my way\", 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "aZ0Yc76rU9kY",
        "outputId": "ce3393c3-815e-4982-c813-cc37326ab00f"
      },
      "execution_count": 327,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the wall was on my way'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 327
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oML-5sJwGRLE"
      },
      "source": [
        "## Justify your decisions\n",
        "\n",
        "Write down justificaitons for your implementation choices. For example, these choices could be:\n",
        "- Which ngram dataset to use\n",
        "- Which weights to assign for edit1, edit2 or absent words probabilities\n",
        "- Beam search parameters\n",
        "- etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Xb_twOmVsC6"
      },
      "source": [
        "The fivegrams is better then bigrams on long sentences, but for this tas long sentences are less likely than short ones (like searching queries). In my implementation, weights for `edit1` and `edit2` are `0.8` and `0.16`, because I consider that probability of two mistakes are twice less that probapility of one mistake. Also in my implementation I take miskliks in attention in `edit_mis` and its have higher probability then `edit1`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46rk65S4GRSe"
      },
      "source": [
        "## Evaluate on a test set\n",
        "\n",
        "Your task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity. Compare your solution to the Norvig's corrector, and report the accuracies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwZWaX9VVs7B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff9a1d0a-44eb-462a-e3ea-6cf833bf7f4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0. This is correct sentence\n",
            "1. This is incoffect sentence\n",
            "Possible replaces:\n",
            "  incorrect (1.1097224350621847e-53)\n",
            "2. Just sent3nse\n"
          ]
        }
      ],
      "source": [
        "test_sentenses = [\n",
        "    'This is correct sentence',\n",
        "    'This is incoffect sentence',\n",
        "    'Just sent3nse',\n",
        "    'This is the emd of testinh'\n",
        "]\n",
        "\n",
        "for i in range(len(test_sentenses)):\n",
        "  print(f\"{i}. {test_sentenses[i]}\")\n",
        "  correct_sentence(test_sentenses[i])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}